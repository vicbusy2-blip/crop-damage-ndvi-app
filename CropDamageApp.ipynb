# app.py
"""
Streamlit app: Crop Damage Assessment (NDVI) — Full pipeline (Phases A -> J)

How to run:
 - Local (interactive EE auth): pip install -r requirements.txt ; streamlit run app.py
 - Colab: run as a python script cell, authenticate with ee.Authenticate()
 - Streamlit Cloud / Server: set GOOGLE_APPLICATION_CREDENTIALS to the service account JSON path
   (or paste credentials into Streamlit secrets and the app will write them to a temp file).

Requirements:
 - earthengine-api, geemap, streamlit, folium, geopandas, shapely, fiona, matplotlib, pandas, numpy
 - Example pip: pip install streamlit earthengine-api geemap folium geopandas shapely fiona matplotlib pandas numpy

Default GeoPackage path (if you uploaded in Colab): /content/colab_CLS2.gpkg
You can upload a .gpkg via the app UI too.
"""

import os
import sys
import time
import shutil
import json
import base64
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import pandas as pd
import geopandas as gpd
from shapely.geometry import mapping

import streamlit as st
import folium
import geemap.foliumap as geemap_tile
import matplotlib.pyplot as plt

# Earth Engine import is lazy (we will try to initialize when needed)
try:
    import ee
except Exception:
    ee = None

# ---------------------------
# Phase A: Parameters (top of file)
# ---------------------------
# File defaults (if running Colab, use uploaded path)
DEFAULT_GPKG_PATH = "/content/colab_CLS2.gpkg"

# User-tunable parameters (edit here or via UI)
PARAMS = {
    # I/O
    "gpkg_path": DEFAULT_GPKG_PATH,
    "export_csv_path": "/content/gee_damage_results_FINAL_RS.csv",
    "export_map_path": "/content/FINAL_CROP_DAMAGE_MAP.html",
    "chart_dir": "/content/charts",  # precomputed charts dir

    # Columns
    "polygon_id_col": "case_ID",
    "loss_date_col": "Date of Lo",

    # Date windows
    "pre_start": "2025-08-01",
    "pre_end": "2025-11-20",
    "post_offset_start_days": 1,
    "post_offset_end_days": 10,

    # Phenology / charts
    "chart_start": "2025-05-15",
    "chart_end": "2025-11-20",
    "chart_freq_days": 15,  # fortnight

    # Threshold / algorithm
    "clear_threshold": 0.8,
    "diff_threshold": 0.16,
    "k_mad": 2,
    "iqr_factor": 1.5,
    "slope_factor": 150,

    # Sampling / spatial / perf
    "scale": 10,
    "maxpix": int(1e7),
    "sample_limit": 3000,
    "buffer_km": 20,

    # performance
    "chart_workers": 6,
    "getinfo_retries": 3,
    "getinfo_wait": 1.3,
}

# ---------------------------
# Helper functions & EE utilities
# ---------------------------

def st_echo(msg):
    st.write(msg)

def safe_get_info(obj, retries=None, wait=None):
    """Retry wrapper around EE getInfo() (or for objects that support getInfo)."""
    if retries is None:
        retries = PARAMS["getinfo_retries"]
    if wait is None:
        wait = PARAMS["getinfo_wait"]
    for attempt in range(1, retries + 1):
        try:
            return obj.getInfo()
        except Exception as e:
            if attempt == retries:
                raise
            time.sleep(wait * attempt)

def convert_3d_to_2d_geojson(geojson_obj):
    """Drop Z/M coordinates (robust). Accepts dict from mapping(geom)."""
    if geojson_obj is None or not isinstance(geojson_obj, dict):
        return geojson_obj
    t = geojson_obj.get('type')
    def strip(obj):
        if isinstance(obj, (list, tuple)):
            if len(obj) >= 2 and isinstance(obj[0], (float, int)):
                return [obj[0], obj[1]]
            return [strip(x) for x in obj]
        return obj
    out = dict(geojson_obj)
    if 'coordinates' in out:
        out['coordinates'] = strip(out['coordinates'])
    if t == 'GeometryCollection' and 'geometries' in out:
        out['geometries'] = [convert_3d_to_2d_geojson(g) for g in out['geometries']]
    return out

# ---------------------------
# EE functions (cloud mask, NDVI, build collections)
# ---------------------------
def ee_authenticate_and_init(service_account_info=None):
    """
    Initialize Earth Engine:
     - If running locally/Colab interactively: call ee.Authenticate() then ee.Initialize()
     - If service_account_info (dict) provided: write JSON -> set GOOGLE_APPLICATION_CREDENTIALS -> ee.Initialize()
    Returns True on success.
    """
    global ee
    if ee is None:
        import importlib
        ee = importlib.import_module('ee')

    # Service account credentials path (Streamlit Cloud / server usage)
    if service_account_info is not None:
        # write to a temporary json file and set env var
        cred_path = os.path.join(os.getcwd(), "sa_creds.json")
        with open(cred_path, "w") as f:
            json.dump(service_account_info, f)
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = cred_path
        ee.Initialize()
        return True

    # If GOOGLE_APPLICATION_CREDENTIALS provided by environment, use it
    if os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
        ee.Initialize()
        return True

    # Interactive authentication (Colab/local)
    try:
        ee.Authenticate()  # will open a link in Colab / local terminal
        ee.Initialize()
        return True
    except Exception as e:
        st.error("Earth Engine authentication failed. Provide service account or run interactive auth.")
        raise

def mask_clouds_with_csp(img, use_csp=True, csp_threshold=None):
    """Masks Sentinel-2 SR using SCL, QA60 and optional Cloud Score+ cs_cdf band."""
    img = ee.Image(img)
    if csp_threshold is None:
        csp_threshold = PARAMS["clear_threshold"]
    bn = img.bandNames()
    scl_mask = ee.Algorithms.If(
        bn.contains('SCL'),
        img.select('SCL').neq(3).And(img.select('SCL').neq(8)).And(img.select('SCL').neq(9)).And(img.select('SCL').neq(10)),
        ee.Image(1)
    )
    qa_mask = ee.Algorithms.If(
        bn.contains('QA60'),
        img.select('QA60').bitwiseAnd(1 << 10).eq(0).And(img.select('QA60').bitwiseAnd(1 << 11).eq(0)),
        ee.Image(1)
    )
    combined = ee.Image(scl_mask).And(ee.Image(qa_mask))

    if use_csp:
        cs_img = ee.Algorithms.If(img.get('cs'), ee.Image(img.get('cs')), None)
        cs_mask = ee.Algorithms.If(
            ee.Algorithms.IsEqual(cs_img, None),
            ee.Image(1),
            ee.Algorithms.If(
                ee.Image(cs_img).bandNames().contains('cs_cdf'),
                ee.Image(cs_img).select('cs_cdf').gte(csp_threshold),
                ee.Image(1)
            )
        )
        combined = combined.And(ee.Image(cs_mask))

    return img.updateMask(combined).copyProperties(img, img.propertyNames())

def add_ndvi(img):
    img = ee.Image(img)
    bn = img.bandNames()
    ndvi_img = ee.Algorithms.If(
        bn.contains('B8'),
        ee.Algorithms.If(bn.contains('B4'), img.normalizedDifference(['B8','B4']).rename('ndvi'),
                         ee.Image.constant(0).rename('ndvi').updateMask(ee.Image.constant(0))),
        ee.Image.constant(0).rename('ndvi').updateMask(ee.Image.constant(0))
    )
    return img.addBands(ee.Image(ndvi_img))

def build_collections(gdf_wgs):
    """Build s2_all (masked + ndvi) and pre_max_ndvi."""
    minx, miny, maxx, maxy = gdf_wgs.total_bounds
    buffer_deg = PARAMS["buffer_km"] / 111.32
    roi_rect = ee.Geometry.Rectangle([minx - buffer_deg, miny - buffer_deg, maxx + buffer_deg, maxy + buffer_deg])

    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
          .filterDate(PARAMS["pre_start"], PARAMS["pre_end"])
          .filterBounds(roi_rect))

    csPlus = (ee.ImageCollection('GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED')
              .filterDate(PARAMS["pre_start"], PARAMS["pre_end"])
              .filterBounds(roi_rect))

    # Try join; if CSP empty it's still safe
    join = ee.Join.saveFirst('cs')
    filter_eq = ee.Filter.equals(leftField='system:index', rightField='system:index')
    s2_with_cs = ee.ImageCollection(join.apply(s2, csPlus, filter_eq))

    s2_all = s2_with_cs.map(lambda img: mask_clouds_with_csp(img)).map(add_ndvi)

    pre_collection = s2_all.filterDate(PARAMS["pre_start"], PARAMS["pre_end"])
    pre_max_ndvi = pre_collection.select('ndvi').max().rename('pre_max_ndvi')

    return s2_all, pre_max_ndvi, roi_rect

# ---------------------------
# Paired sampling and metrics (kept logic intact; renamed almost verbatim)
# ---------------------------
def sample_paired_pixels(pre_img, post_img, ee_geom, sample_limit=None, scale=None):
    if sample_limit is None:
        sample_limit = PARAMS["sample_limit"]
    if scale is None:
        scale = PARAMS["scale"]

    # handle empty post image case
    try:
        post_bands = []
        try:
            post_bands = safe_get_info(ee.Image(post_img).bandNames())
        except Exception:
            post_bands = []
        if not post_bands:
            samp = ee.Image(pre_img).rename('pre').sample(region=ee_geom, scale=scale, numPixels=sample_limit, seed=42)
            pre_arr = safe_get_info(samp.aggregate_array('pre')) or []
            pre_vals = [float(v) for v in pre_arr if v is not None and isinstance(v, (int, float))]
            return pre_vals, []
    except Exception:
        pass

    paired = ee.Image.cat(ee.Image(pre_img).rename('pre'), ee.Image(post_img).rename('post'))
    fc = paired.sample(region=ee_geom, scale=scale, numPixels=sample_limit, seed=42)

    try:
        pre_arr = safe_get_info(fc.aggregate_array('pre')) or []
        post_arr = safe_get_info(fc.aggregate_array('post')) or []
    except Exception:
        # fallback to separate sampling
        try:
            pre_arr = safe_get_info(ee.Image(pre_img).sample(region=ee_geom, scale=scale, numPixels=min(sample_limit,1000), seed=43).aggregate_array(pre_img.bandNames().get(0))) or []
            post_arr = safe_get_info(ee.Image(post_img).sample(region=ee_geom, scale=scale, numPixels=min(sample_limit,1000), seed=44).aggregate_array(post_img.bandNames().get(0))) or []
        except Exception:
            pre_arr, post_arr = [], []

    pre_vals = [float(v) for v in pre_arr if v is not None and isinstance(v, (int, float))]
    post_vals = [float(v) for v in post_arr if v is not None and isinstance(v, (int, float))]
    # ensure matched
    n = min(len(pre_vals), len(post_vals))
    if n == 0:
        return pre_vals[:n], post_vals[:n]
    return pre_vals[:n], post_vals[:n]

def compute_severity_and_metrics(pre_vec, post_vec, scale=None):
    """Uses same algorithm/decision tree as your reference. Returns dict of metrics."""
    if scale is None:
        scale = PARAMS["scale"]

    pre = np.array(pre_vec, dtype=float)
    post = np.array(post_vec, dtype=float)
    n = min(pre.size, post.size)
    if n == 0:
        return {
            "fixed_ha": 0.0, "mad_ha": 0.0, "iqr_ha": 0.0,
            "RS_AffectedArea": 0.0, "LogicFlag": "No matched pixels",
            "RS_Severity": None, "RS_Severity_Affected": None,
            "mean_relative_severity_pct": None, "std_relative_severity_pct": None,
            "mean_normalized_severity_pct": None, "std_normalized_severity_pct": None,
            "mean_scaled_severity_pct": None, "std_scaled_severity_pct": None,
            "mean_severity_affected_relative": None, "std_severity_affected_relative": None
        }

    pre_m = pre[:n]
    post_m = post[:n]

    median_pre = float(np.nanmedian(pre_m)) if np.isfinite(np.nanmedian(pre_m)) else 0.0
    mad = float(np.nanmedian(np.abs(pre_m - median_pre))) if np.isfinite(np.nanmedian(pre_m - median_pre)) else 0.0
    q1 = float(np.nanpercentile(pre_m, 25)) if pre_m.size > 0 else 0.0
    q3 = float(np.nanpercentile(pre_m, 75)) if pre_m.size > 0 else 0.0
    iqr = q3 - q1

    spread = 1.2 * (mad * 1.4826)
    thr_mad = median_pre - PARAMS["k_mad"] * spread if np.isfinite(median_pre) else np.nan
    thr_iqr = q1 - PARAMS["iqr_factor"] * iqr if np.isfinite(q1) else np.nan

    fixed_mask = (pre_m - post_m) >= PARAMS["diff_threshold"]
    mad_mask = np.full_like(post_m, False, dtype=bool) if np.isnan(thr_mad) else (post_m <= thr_mad)
    iqr_mask = np.full_like(post_m, False, dtype=bool) if np.isnan(thr_iqr) else (post_m <= thr_iqr)

    fixed_count = int(np.nansum(fixed_mask))
    mad_count = int(np.nansum(mad_mask))
    iqr_count = int(np.nansum(iqr_mask))

    ha_per_pixel = (scale * scale) / 10000.0
    fixed_ha = round(fixed_count * ha_per_pixel, 4)
    mad_ha = round(mad_count * ha_per_pixel, 4)
    iqr_ha = round(iqr_count * ha_per_pixel, 4)
    total_ha = round(n * ha_per_pixel, 4)

    with np.errstate(divide='ignore', invalid='ignore'):
        sev_rel = np.clip((pre_m - post_m) / pre_m, 0, None)
    sev_rel = np.nan_to_num(sev_rel, nan=0.0, posinf=0.0)
    sev_rel_pct = np.clip(sev_rel * 100, 0, 100)

    sev_norm_med = np.clip((pre_m - post_m) / median_pre, 0, None) if median_pre != 0 else np.zeros_like(pre_m)
    sev_norm_med_pct = np.clip(sev_norm_med * 100, 0, 100)

    sev_scaled = PARAMS["slope_factor"] * np.clip(pre_m - post_m, 0, None)
    sev_scaled_pct = np.clip(sev_scaled, 0, 100)

    mean_rel_all = float(np.nanmean(sev_rel_pct))
    std_rel_all = float(np.nanstd(sev_rel * 100))
    mean_norm_all = float(np.nanmean(sev_norm_med_pct))
    std_norm_all = float(np.nanstd(sev_norm_med * 100))
    mean_scaled_all = float(np.nanmean(sev_scaled_pct))
    std_scaled_all = float(np.nanstd(sev_scaled))

    affected_mask = fixed_mask | mad_mask | iqr_mask
    if np.sum(affected_mask) > 0:
        mean_rel_aff = float(np.nanmean(sev_rel_pct[affected_mask]))
        std_rel_aff = float(np.nanstd(sev_rel[affected_mask] * 100))
        mean_norm_aff = float(np.nanmean(sev_norm_med_pct[affected_mask]))
        std_norm_aff = float(np.nanstd(sev_norm_med[affected_mask] * 100))
        mean_scaled_aff = float(np.nanmean(sev_scaled_pct[affected_mask]))
        std_scaled_aff = float(np.nanstd(sev_scaled[affected_mask]))
    else:
        mean_rel_aff = std_rel_aff = mean_norm_aff = std_norm_aff = mean_scaled_aff = std_scaled_aff = None

    def format_sev(mean, std):
        if mean is None or np.isnan(mean):
            return None
        if mean >= 100:
            return "100"
        else:
            return f"{int(round(mean))} [+/- {int(round(std))}]"

    RS_Severity_all = format_sev(mean_rel_all, std_rel_all)
    RS_Severity_affected = format_sev(mean_rel_aff, std_rel_aff) if mean_rel_aff is not None else None

    # Decision tree for final affected area (same logic)
    fixed_ha_val = fixed_ha if not np.isnan(fixed_ha) else 0
    mad_ha_val = mad_ha if not np.isnan(mad_ha) else 0
    iqr_ha_val = iqr_ha if not np.isnan(iqr_ha) else 0

    if total_ha == 0:
        final_aff_area = 0.0; logic_flag = "No matched pixels"
    elif fixed_ha_val >= 0.85 * total_ha:
        final_aff_area = fixed_ha_val; logic_flag = "Fixed_HA Dominant"
    elif all(x < 0.1 * total_ha for x in [fixed_ha_val, mad_ha_val, iqr_ha_val]):
        final_aff_area = max(fixed_ha_val, mad_ha_val, iqr_ha_val); logic_flag = "All Low (<10%)"
    elif mad_ha_val >= 0.9 * total_ha and iqr_ha_val >= 0.9 * total_ha:
        final_aff_area = round((0.9 * total_ha + 0.9 * total_ha) / 2, 4); logic_flag = "MAD & IQR High (90%)"
    elif abs(mad_ha_val - iqr_ha_val) > 0.5 * total_ha:
        final_aff_area = max(fixed_ha_val, mad_ha_val, iqr_ha_val); logic_flag = "Diverse Results"
    elif 0.3 * total_ha <= mad_ha_val <= 0.9 * total_ha and 0.3 * total_ha <= iqr_ha_val <= 0.9 * total_ha:
        final_aff_area = round((mad_ha_val + iqr_ha_val) / 2, 4); logic_flag = "MAD & IQR Moderate"
    elif mad_ha_val < 0.3 * total_ha and iqr_ha_val < 0.3 * total_ha:
        final_aff_area = max(fixed_ha_val, mad_ha_val, iqr_ha_val); logic_flag = "MAD & IQR Low (<30%)"
    else:
        final_aff_area = max(fixed_ha_val, mad_ha_val, iqr_ha_val); logic_flag = "Fallback Max"

    return {
        "fixed_ha": fixed_ha, "mad_ha": mad_ha, "iqr_ha": iqr_ha,
        "RS_AffectedArea": final_aff_area, "LogicFlag": logic_flag,
        "RS_Severity": RS_Severity_all, "RS_Severity_Affected": RS_Severity_affected,
        "mean_relative_severity_pct": round(mean_rel_all, 2),
        "std_relative_severity_pct": round(std_rel_all, 2),
        "mean_normalized_severity_pct": round(mean_norm_all, 2),
        "std_normalized_severity_pct": round(std_norm_all, 2),
        "mean_scaled_severity_pct": round(mean_scaled_all, 2),
        "std_scaled_severity_pct": round(std_scaled_all, 2),
        "mean_severity_affected_relative": round(mean_rel_aff, 2) if mean_rel_aff is not None else None,
        "std_severity_affected_relative": round(std_rel_aff, 2) if std_rel_aff is not None else None
    }

# ---------------------------
# Chart generation helpers
# ---------------------------
def make_chart_and_save(dates, ndvi_vals, out_path):
    """Plot NDVI line and save PNG to out_path."""
    fig, ax = plt.subplots(figsize=(4.2, 2.6))
    ax.plot(dates, ndvi_vals, marker='o', linestyle='-', color='green')
    ax.set_ylim(0, 1)
    ax.set_xlabel("Date", fontsize=9)
    ax.set_ylabel("NDVI", fontsize=9)
    ax.tick_params(axis='x', labelrotation=45, labelsize=8)
    ax.grid(True, linewidth=0.3)
    plt.tight_layout()
    fig.savefig(out_path, dpi=110)
    plt.close(fig)
    return out_path

def png_to_base64(path):
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

# ---------------------------
# Streamlit UI: main app
# ---------------------------
st.set_page_config(layout="wide", page_title="Crop Damage Assessment (NDVI)")

st.title("Crop Damage Assessment — NDVI (Full Pipeline: Phases A → J)")

# Sidebar: authentication + upload + params
st.sidebar.header("1) Authentication & Data")
auth_choice = st.sidebar.selectbox("EE Auth method", ("Interactive (Colab/local)", "Service account JSON (server)"))

if auth_choice.startswith("Service"):
    st.sidebar.info("Paste your Service Account JSON below (or set env var GOOGLE_APPLICATION_CREDENTIALS).")
    sa_text = st.sidebar.text_area("Service account JSON (raw)", height=200)
else:
    sa_text = None

# GPKG upload
uploaded = st.sidebar.file_uploader("Upload GeoPackage (.gpkg)", type=["gpkg"])
if uploaded:
    # write to a temp path
    gpkg_path_runtime = os.path.join("/tmp", uploaded.name)
    with open(gpkg_path_runtime, "wb") as f:
        f.write(uploaded.getbuffer())
    PARAMS["gpkg_path"] = gpkg_path_runtime
    st.sidebar.success(f"Uploaded: {uploaded.name}")
else:
    # use default path if exists
    if os.path.exists(PARAMS["gpkg_path"]):
        st.sidebar.info(f"Using default GPKG: {PARAMS['gpkg_path']}")
    else:
        st.sidebar.warning("No GeoPackage found in default path. Upload one or place at the default path.")

# Parameters block (collapsible)
with st.sidebar.expander("2) Parameters (Edit and re-run)"):
    PARAMS["pre_start"] = st.date_input("Pre start", pd.to_datetime(PARAMS["pre_start"])).strftime("%Y-%m-%d")
    PARAMS["pre_end"] = st.date_input("Pre end", pd.to_datetime(PARAMS["pre_end"])).strftime("%Y-%m-%d")
    PARAMS["post_offset_start_days"] = st.number_input("Post offset start days", value=PARAMS["post_offset_start_days"], min_value=0)
    PARAMS["post_offset_end_days"] = st.number_input("Post offset end days", value=PARAMS["post_offset_end_days"], min_value=0)
    PARAMS["chart_start"] = st.date_input("Chart start", pd.to_datetime(PARAMS["chart_start"])).strftime("%Y-%m-%d")
    PARAMS["chart_end"] = st.date_input("Chart end", pd.to_datetime(PARAMS["chart_end"])).strftime("%Y-%m-%d")
    PARAMS["chart_freq_days"] = st.number_input("Chart freq days", value=PARAMS["chart_freq_days"], min_value=1)
    PARAMS["sample_limit"] = st.number_input("Per-polygon sample_limit", value=PARAMS["sample_limit"], min_value=100, max_value=10000, step=100)
    PARAMS["chart_workers"] = st.number_input("Chart generator threads", value=PARAMS["chart_workers"], min_value=1, max_value=16)

# Buttons: init EE, run pipeline
col1, col2 = st.columns(2)
with col1:
    if st.button("Initialize Earth Engine"):
        try:
            if sa_text and auth_choice.startswith("Service"):
                sa_dict = json.loads(sa_text)
                ee_authenticate_and_init(service_account_info=sa_dict)
            else:
                ee_authenticate_and_init(service_account_info=None)
            st.success("Earth Engine initialized successfully.")
        except Exception as e:
            st.error(f"EE init failed: {e}")
with col2:
    run_pipeline = st.button("Run full pipeline (Phases C->J)")

# Show uploaded file info
st.sidebar.header("GPKG info")
gpkg_path_show = PARAMS["gpkg_path"]
if os.path.exists(gpkg_path_show):
    st.sidebar.write(gpkg_path_show)
    try:
        tmp_gdf = gpd.read_file(gpkg_path_show)
        st.sidebar.write(f"Polygons: {len(tmp_gdf)}")
    except Exception:
        st.sidebar.write("Could not read geodata (check file).")
else:
    st.sidebar.write("No GeoPackage available at default path. Upload in left panel.")

# If run_pipeline clicked: run heavy tasks
if run_pipeline:
    # Step 1: check gpkg
    if not os.path.exists(PARAMS["gpkg_path"]):
        st.error("GeoPackage not found. Upload file or place at default path.")
        st.stop()

    # Attempt EE init if not already
    if ee is None:
        try:
            if sa_text and auth_choice.startswith("Service"):
                sa_dict = json.loads(sa_text)
                ee_authenticate_and_init(service_account_info=sa_dict)
            else:
                ee_authenticate_and_init(service_account_info=None)
            st.success("Earth Engine ready.")
        except Exception as e:
            st.error(f"EE initialization error: {e}")
            st.stop()

    # Phase B: read geopackage
    st.info("Phase B: Reading GeoPackage & preparing geometries...")
    gdf = gpd.read_file(PARAMS["gpkg_path"])
    if PARAMS["loss_date_col"] in gdf.columns:
        gdf[PARAMS["loss_date_col"]] = pd.to_datetime(gdf[PARAMS["loss_date_col"]], errors='coerce')
    gdf_wgs = gdf.to_crs(epsg=4326)
    st.write(f"Loaded {len(gdf_wgs)} polygons.")

    # Phase C: build collections
    st.info("Phase C: Building S2 collections (this may take a few seconds)...")
    try:
        s2_all, pre_max_ndvi, roi_rect = build_collections(gdf_wgs)
        st.success("Phase C complete (collections ready).")
    except Exception as e:
        st.error(f"Error building collections: {e}")
        st.stop()

    # Phase F: main processing loop (paired sampling) — show progress bar
    st.info("Phase F: Running per-polygon paired sampling and metrics (this may take time depending on polygon count)...")
    results = []
    total = len(gdf_wgs)
    progress = st.progress(0)
    start_time = time.time()

    for idx, row in gdf_wgs.iterrows():
        case_id = row.get(PARAMS["polygon_id_col"])
        loss_date = pd.to_datetime(row.get(PARAMS["loss_date_col"]), errors='coerce') if PARAMS["loss_date_col"] in row.index else None

        geom = row.geometry.buffer(0)
        if geom is None or geom.is_empty:
            results.append({"case_ID": case_id, "LogicFlag": "Invalid Geometry", "RS_AffectedArea": 0})
            progress.progress(int((idx+1)/total * 100))
            continue

        gj = mapping(geom)
        gj2d = convert_3d_to_2d_geojson(dict(gj))
        ee_geom = ee.Geometry(gj2d)

        # build post image for this polygon (median)
        if not pd.isna(loss_date):
            start = (loss_date + pd.Timedelta(days=PARAMS["post_offset_start_days"])).strftime("%Y-%m-%d")
            end = (loss_date + pd.Timedelta(days=PARAMS["post_offset_end_days"])).strftime("%Y-%m-%d")
            post_ic = s2_all.filterDate(start, end)
            try:
                n_post_images = int(safe_get_info(post_ic.size()))
            except Exception:
                n_post_images = 0
            post_img = post_ic.median().select('ndvi') if n_post_images > 0 else ee.Image([])
            post_range = f"{start}..{end}" if n_post_images > 0 else None
        else:
            n_post_images = 0
            post_img = ee.Image([])
            post_range = None

        pre_img = pre_max_ndvi.select('pre_max_ndvi') if 'pre_max_ndvi' in safe_get_info(pre_max_ndvi.bandNames()) else pre_max_ndvi

        pre_vals, post_vals = sample_paired_pixels(pre_img, post_img, ee_geom, sample_limit=PARAMS["sample_limit"], scale=PARAMS["scale"])

        pre_pixel_count = len(pre_vals)
        post_pixel_count = len(post_vals)
        used_pixels = min(pre_pixel_count, post_pixel_count)
        Surveyed_Area = round(pre_pixel_count * 0.01, 4)

        metrics = compute_severity_and_metrics(pre_vals, post_vals, scale=PARAMS["scale"])

        row_out = {
            "case_ID": case_id,
            "Loss_Date": loss_date.date() if not pd.isna(loss_date) else None,
            "Post_Date_Range": post_range,
            "n_post_images": n_post_images,
            "post_cloud_pct": None,
            "pre_AvgNDVI": round(float(np.nanmean(pre_vals)), 3) if pre_vals else None,
            "post_AvgNDVI": round(float(np.nanmean(post_vals)), 3) if post_vals else None,
            "preImage Pixels": pre_pixel_count,
            "postImage Pixels": post_pixel_count,
            "usedPixels": int(used_pixels),
            "Surveyed_Area": Surveyed_Area
        }
        row_out.update(metrics)
        results.append(row_out)

        progress.progress(int((idx+1)/total * 100))

    elapsed = time.time() - start_time
    st.success(f"Phase F complete — processed {total} polygons in {elapsed:.1f} s (approx).")

    # Save CSV and show download link
    df = pd.DataFrame(results)
    # ensure desired columns exist (keeps same shape as before)
    desired_cols = [
        "case_ID","Loss_Date","Post_Date_Range","n_post_images","post_cloud_pct",
        "pre_AvgNDVI","post_AvgNDVI",
        "preImage Pixels","postImage Pixels","usedPixels","Surveyed_Area",
        "fixed_ha","mad_ha","iqr_ha",
        "RS_AffectedArea","LogicFlag","RS_Severity","RS_Severity_Affected",
        "mean_relative_severity_pct","std_relative_severity_pct",
        "mean_normalized_severity_pct","std_normalized_severity_pct",
        "mean_scaled_severity_pct","std_scaled_severity_pct",
        "mean_severity_affected_relative","std_severity_affected_relative"
    ]
    for c in desired_cols:
        if c not in df.columns:
            df[c] = None
    df = df[desired_cols]
    csv_path = PARAMS["export_csv_path"]
    df.to_csv(csv_path, index=False, float_format="%.4f")
    st.success(f"CSV saved: {csv_path}")
    with open(csv_path, "rb") as f:
        st.download_button("Download results CSV", f, file_name=os.path.basename(csv_path))

    # Phase G: Precompute charts (multithreaded)
    st.info("Phase G: Precomputing NDVI charts for popups (multithreaded). This speeds up map popups.")
    chart_dir = PARAMS["chart_dir"]
    if os.path.exists(chart_dir):
        shutil.rmtree(chart_dir)
    os.makedirs(chart_dir, exist_ok=True)

    # Build s2_chart collection (fortnights)
    fortnights = pd.date_range(PARAMS["chart_start"], PARAMS["chart_end"], freq=f"{PARAMS['chart_freq_days']}D")
    s2_chart_base = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterDate(PARAMS["chart_start"], PARAMS["chart_end"])
                     .filterBounds(roi_rect))
    csPlus_chart = (ee.ImageCollection('GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED')
                    .filterDate(PARAMS["chart_start"], PARAMS["chart_end"])
                    .filterBounds(roi_rect))
    join = ee.Join.saveFirst('cs')
    cond = ee.Filter.equals(leftField='system:index', rightField='system:index')
    s2WithCS = ee.ImageCollection(join.apply(s2_chart_base, csPlus_chart, cond))
    s2_chart = s2WithCS.map(lambda img: mask_clouds_with_csp(img)).map(add_ndvi).map(lambda i: i.set('date', ee.Date(i.get('system:time_start')).format('YYYY-MM-dd')))

    # function to compute one chart (for multithreading)
    def compute_and_save_chart(case_row):
        case_id = str(case_row.get(PARAMS["polygon_id_col"]))
        geom = case_row.geometry.buffer(0)
        gj = mapping(geom)
        gj2d = convert_3d_to_2d_geojson(dict(gj))
        ee_geom = ee.Geometry(gj2d)
        ts = []
        for i in range(len(fortnights)-1):
            s = fortnights[i].strftime("%Y-%m-%d")
            e = fortnights[i+1].strftime("%Y-%m-%d")
            ic = s2_chart.filterDate(s, e).select('ndvi')
            try:
                size = safe_get_info(ic.size())
                if size == 0:
                    val = np.nan
                else:
                    val = safe_get_info(ic.max().reduceRegion(ee.Reducer.mean(), ee_geom, PARAMS["scale"], maxPixels=PARAMS["maxpix"]).get('ndvi'))
            except Exception:
                val = np.nan
            ts.append({"date": s, "maxNDVI": val})
        df_poly = pd.DataFrame(ts)
        df_poly["date"] = pd.to_datetime(df_poly["date"])
        df_poly["maxNDVI"] = df_poly["maxNDVI"].fillna(0)
        out_path = os.path.join(chart_dir, f"{case_id}.png")
        make_chart_and_save(df_poly["date"], df_poly["maxNDVI"], out_path)
        return case_id, out_path

    # run threaded
    workers = max(1, int(PARAMS["chart_workers"]))
    chart_paths = {}
    with ThreadPoolExecutor(max_workers=workers) as exe:
        futures = {exe.submit(compute_and_save_chart, row): idx for idx, row in gdf_wgs.iterrows()}
        pbar = st.progress(0)
        total_f = len(futures)
        done = 0
        for fut in as_completed(futures):
            try:
                cid, path = fut.result()
                chart_paths[cid] = path
            except Exception as ex:
                st.warning(f"Chart generation error: {ex}")
            done += 1
            pbar.progress(int(done/total_f * 100))

    st.success(f"Charts precomputed for {len(chart_paths)} polygons (saved to {chart_dir}).")

    # Phase I+J: Build Folium map (fast) with single FeatureGroup for polygons
    st.info("Phase I+J: Building fast folium map (EE tiles + single polygon layer).")
    centroid = roi_rect.centroid().coordinates().getInfo()
    m = folium.Map(location=[centroid[1], centroid[0]], zoom_start=11, tiles="Esri.WorldImagery")

    # EE tile layers
    rgb_vis = {"min":0, "max":3000, "bands":['B4','B3','B2']}
    ndvi_vis = {"min":0.0, "max":0.9, "palette":["red","yellow","green"]}

    pre_rgb = s2_all.filterDate(PARAMS["pre_start"], PARAMS["pre_end"]).median().select(['B4','B3','B2'])
    m.add_child(geemap_tile.ee_tile_layer(pre_rgb, vis_params=rgb_vis, name="Pre-event RGB"))

    m.add_child(geemap_tile.ee_tile_layer(pre_max_ndvi, vis_params=ndvi_vis, name="Pre-event Max NDVI"))

    # Add post-event RGBs only for unique loss dates
    loss_dates = pd.to_datetime(gdf_wgs[PARAMS["loss_date_col"]].dropna(), errors='coerce').unique()
    for ld in loss_dates:
        if pd.isna(ld):
            continue
        ld = pd.to_datetime(ld)
        start = (ld + pd.Timedelta(days=PARAMS["post_offset_start_days"])).strftime("%Y-%m-%d")
        end = (ld + pd.Timedelta(days=PARAMS["post_offset_end_days"])).strftime("%Y-%m-%d")
        post_ic = s2_all.filterDate(start, end)
        try:
            if safe_get_info(post_ic.size()) > 0:
                post_rgb = post_ic.median().select(['B4','B3','B2'])
                m.add_child(geemap_tile.ee_tile_layer(post_rgb, vis_params=rgb_vis, name=f"Post-event RGB {start}..{end}"))
        except Exception:
            continue

    # Single FeatureGroup for polygons
    poly_group = folium.FeatureGroup(name="CLS Polygons", show=True)
    m.add_child(poly_group)

    # Build merged display gdf for popups (merge df produced earlier)
    gdf_display = gdf_wgs.merge(df, left_on=PARAMS["polygon_id_col"], right_on="case_ID", how="left")

    # Add polygons into the single FeatureGroup; popups link to precomputed PNG (embedded as base64 inline)
    for _, row in gdf_display.iterrows():
        case_id = str(row.get("case_ID", ""))
        geom_json = mapping(row.geometry)
        geom_json_2d = convert_3d_to_2d_geojson(dict(geom_json))
        chart_file = chart_paths.get(case_id)
        if chart_file and os.path.exists(chart_file):
            chart_b64 = png_to_base64(chart_file)
            chart_tag = f'<img src="data:image/png;base64,{chart_b64}" width="300px">'
        else:
            chart_tag = "<i>No chart available</i>"

        # build popup HTML showing requested fields
        html = f"""
        <div style="width:360px; font-size:13px; line-height:1.3;">
          <b>Case ID:</b> {row.get('case_ID','N/A')}<br>
          <b>Sowing Date:</b> {row.get('Sowing Dat','N/A')}<br>
          <b>Loss Date:</b> {row.get('Date of Lo','N/A')}<br>
          <b>Cause of Loss:</b> {row.get('Cause of e','N/A')}<br>
          <b>Affected Area (Survey, ha):</b> {row.get('Area affec','N/A')}<br>
          <b>Crop Loss (Survey, %):</b> {row.get('Loss %','N/A')}<br><br>

          <b>preEvent NDVI:</b> {row.get('pre_AvgNDVI','N/A')}<br>
          <b>postEvent NDVI:</b> {row.get('post_AvgNDVI','N/A')}<br>
          <b>Used Pixels:</b> {row.get('usedPixels','N/A')}<br>
          <b>Surveyed Area (ha):</b> {row.get('Surveyed_Area','N/A')}<br><br>

          <b>Affected Area (RS, ha):</b> {row.get('RS_AffectedArea','N/A')}<br>
          <b>Crop Loss (RS, %):</b> {row.get('RS_Severity','N/A')}<br><br>

          <b>NDVI Phenology</b><br>
          {chart_tag}
        </div>
        """
        iframe = folium.IFrame(html=html, width=370, height=460)
        popup = folium.Popup(iframe, max_width=400)

        folium.GeoJson(
            geom_json_2d,
            popup=popup,
            tooltip=f"{case_id}",
            style_function=lambda feat: {'color': 'orange', 'weight': 2, 'fillOpacity': 0.12}
        ).add_to(poly_group)

    folium.LayerControl(collapsed=False).add_to(m)

    # Save map to disk (HTML)
    map_out = PARAMS["export_map_path"]
    m.save(map_out)
    st.success(f"Map built and saved: {map_out}")

    # Display map in Streamlit
    st.components.v1.html(m.get_root().render(), height=700, scrolling=True)

    # Provide search box to center on polygon (simple implementation)
    st.markdown("---")
    st.header("Search polygons by Case ID")
    sid = st.text_input("Enter case_ID to zoom (exact match):")
    if sid:
        match = gdf_display[gdf_display["case_ID"].astype(str) == sid]
        if len(match) == 0:
            st.warning("No polygon found with that case_ID")
        else:
            geom = mapping(match.iloc[0].geometry)
            centroid = match.iloc[0].geometry.centroid
            # build a small map with only selected polygon highlighted
            m2 = folium.Map(location=[centroid.y, centroid.x], zoom_start=14, tiles="Esri.WorldImagery")
            folium.GeoJson(convert_3d_to_2d_geojson(dict(geom)), style_function=lambda f: {'color':'red','weight':3,'fillOpacity':0.25}).add_to(m2)
            st.components.v1.html(m2.get_root().render(), height=500)

    st.balloons()
    st.success("Pipeline run complete. You can download the CSV and the map HTML above.")
